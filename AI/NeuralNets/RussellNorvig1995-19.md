# 19. Learning in neural and belief networks

\[Notes from §19 of *Artificial Intelligence: A Modern Approach* by Stuart Russell and Peter Norvig (1995)\]

Contents:
- [How the brain works](#how-the-brain-works)
- [Neural networks](#neural-networks)
- [Perceptrons](#perceptrons)
- [Multilayer feed-forward networks](#multilayer-feed-forward-networks)
- [Applications of neural networks](#applications-of-neural-networks)
- [Bayesian methods for learning belief networks](#bayesian-methods-for-learning-belief-networks)

§0.1. A neural network is a network of simple arithmetic computing elements that represents a function (just as circuits of simple logic gates represent Boolean functions). Neural networks are “particularly useful for complex functions with continuous-valued outputs and large numbers of noisy inputs”.

§0.2. Other names for neural networks are: connectionism, parallel distributed processing, neural computation, adaptive networks, collective computation.

## How the brain works

§1.1. Brains consist of neurons. Neurons have a soma, dendrites, and an axon. Axons transmit an action potential to synapses. Synapses can be excitatory or inhibitatory.

§1.2. “Synaptic connections exhibit <mark>plasticity</mark> – long-term changes in the strength of the connections in response to the pattern of stimulation. Neurons also form new connections with other neurons, and sometimes entire collections of neurons can migrate from one place to another. These mechanisms are thought to form the basis for learning in the brain.”

§1.3. It is known that certain areas of the brain have specific *functions*:
- Pierre Paul <mark>Broca</mark> (1861) demonstrated that the third left frontal convolution of the cerebral cortex is important for speech and language (cf. aphasia).

§1.4. Brains cause minds! The only alternative theory is mysticism.

### Comparing brains with digital computers

§1.5. Computers have a much higher ‘switching speed’ (time to execute instructions) than do brains (nanoseconds versus milliseconds).

§1.6. However, brains are <mark>massively parallel</mark>, whereas computer hardware is serial.

§1.7. Therefore, brain are faster than computers at what they do.

§1.8. Brains are more <mark>fault tolerant</mark> than computers.

§1.9. Perhaps, artificial neural networks offer a way of building a massively parallel computer, which is more fault tolerant, has <mark>graceful degradation</mark> (performance drops off gradually rather than sharply as conditions worsen), and can do <mark>inductive learning</mark> from input/output pairs.

Back up to: [Top](#)

## Neural networks

### Simple computing elements

### Network structures

### Optimal network structure

Back up to: [Top](#)

## Perceptrons

### What perceptrons can represent

### Learning linearly separable functions

Back up to: [Top](#)

## Multilayer feed-forward networks

### Back-propagation learning

### Back-propagation as gradient descent search

### Discussion

Back up to: [Top](#)

## Applications of neural networks

### Pronunciation

### Handwritten character recognition

### Driving

Back up to: [Top](#)

## Bayesian methods for learning belief networks

### Bayesian learning

### Belief network learning problems

### Learning networks with fixed structure

### A comparison of belief networks and neural networks

Back up to: [Top](#)

----

Back up to: [Neural networks](index.md) | [Artificial intelligence](../index.md)
