# Learning in neural and belief networks

\[Notes from ยง19 of *Artificial Intelligence: A Modern Approach* by Stuart Russell and Peter Norvig (1995)\]

Contents:
- [How the brain works](#how-the-brain-works)
- [Neural networks](#neural-networks)
- [Perceptrons](#perceptrons)
- [Multilayer feed-forward networks](#multilayer-feed-forward-networks)
- [Applications of neural networks](#applications-of-neural-networks)
- [Bayesian methods for learning belief networks](#bayesian-methods-for-learning-belief-networks)



## How the brain works

### Comparing brains with digital computers

## Neural networks

### Simple computing elements

### Network structures

### Optimal network structure

## Perceptrons

### What perceptrons can represent

### Learning linearly separable functions

## Multilayer feed-forward networks

### Back-propagation learning

### Back-propagation as gradient descent search

### Discussion

## Applications of neural networks

### Pronunciation

### Handwritten character recognition

### Driving

## Bayesian methods for learning belief networks

### Bayesian learning

### Belief network learning problems

### Learning networks with fixed structure

### A comparison of belief networks and neural networks


----

Back up to: [Neural networks](index.md) | [Artificial intelligence](../index.md)
