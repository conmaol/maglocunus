# 19. Learning in neural and belief networks

\[Notes from §19 of *Artificial Intelligence: A Modern Approach* by Stuart Russell and Peter Norvig (1995)\]

Contents:
- [How the brain works](#how-the-brain-works)
- [Neural networks](#neural-networks)
- [Perceptrons](#perceptrons)
- [Multilayer feed-forward networks](#multilayer-feed-forward-networks)
- [Applications of neural networks](#applications-of-neural-networks)
- [Bayesian methods for learning belief networks](#bayesian-methods-for-learning-belief-networks)

§0.1. A neural network is a network of simple arithmetic computing elements that represents a function (just as circuits of simple logic gates represent Boolean functions). Neural networks are “particularly useful for complex functions with continuous-valued outputs and large numbers of noisy inputs”.

§0.2. Other names for neural networks are: connectionism, parallel distributed processing, neural computation, adaptive networks, collective computation.

## 19-1. How the brain works

§1.1. Brains consist of neurons. Neurons have a soma, dendrites, and an axon. Axons transmit an action potential to synapses. Synapses can be excitatory or inhibitatory.

§1.2. “Synaptic connections exhibit <mark>plasticity</mark> – long-term changes in the strength of the connections in response to the pattern of stimulation. Neurons also form new connections with other neurons, and sometimes entire collections of neurons can migrate from one place to another. These mechanisms are thought to form the basis for learning in the brain.”

§1.3. It is known that certain areas of the brain have specific *functions*:
- Pierre Paul <mark>Broca</mark> (1861) demonstrated that the third left frontal convolution of the cerebral cortex is important for speech and language (cf. aphasia).

§1.4. Brains cause minds! The only alternative theory is mysticism.

### Comparing brains with digital computers

## 19-2. Neural networks

### Simple computing elements

### Network structures

### Optimal network structure

## Perceptrons

### What perceptrons can represent

### Learning linearly separable functions

## Multilayer feed-forward networks

### Back-propagation learning

### Back-propagation as gradient descent search

### Discussion

## Applications of neural networks

### Pronunciation

### Handwritten character recognition

### Driving

## Bayesian methods for learning belief networks

### Bayesian learning

### Belief network learning problems

### Learning networks with fixed structure

### A comparison of belief networks and neural networks


----

Back up to: [Neural networks](index.md) | [Artificial intelligence](../index.md)
