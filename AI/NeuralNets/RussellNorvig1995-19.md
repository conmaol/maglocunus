# Learning in neural and belief networks

\[Notes from §19 of *Artificial Intelligence: A Modern Approach* by Stuart Russell and Peter Norvig (1995)\]

Contents:
- [How the brain works](#how-the-brain-works)
- [Neural networks](#neural-networks)
- [Perceptrons](#perceptrons)
- [Multilayer feed-forward networks](#multilayer-feed-forward-networks)
- [Applications of neural networks](#applications-of-neural-networks)
- [Bayesian methods for learning belief networks](#bayesian-methods-for-learning-belief-networks)

A neural network is a network of simple arithmetic computing elements that represents a function (just as circuits of simple logic gates represent Boolean functions). Neural networks are “particularly useful for complex functions with continuous-valued outputs and large numbers of noisy inputs”.

Other names for neural networks are: connectionism, parallel distributed processing, neural computation, adaptive networks, collective computation.

## How the brain works

### Comparing brains with digital computers

## Neural networks

### Simple computing elements

### Network structures

### Optimal network structure

## Perceptrons

### What perceptrons can represent

### Learning linearly separable functions

## Multilayer feed-forward networks

### Back-propagation learning

### Back-propagation as gradient descent search

### Discussion

## Applications of neural networks

### Pronunciation

### Handwritten character recognition

### Driving

## Bayesian methods for learning belief networks

### Bayesian learning

### Belief network learning problems

### Learning networks with fixed structure

### A comparison of belief networks and neural networks


----

Back up to: [Neural networks](index.md) | [Artificial intelligence](../index.md)
