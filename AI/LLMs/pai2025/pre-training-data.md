# Designing LLM Applications – Pre-training data

Contents:
- [Ingredients of an LLM](#ingredients-of-an-llm)
- [Pre-training data requirements](#pre-training-data-requirements)
- [Popular pre-training datasets](#popular-pre-training-datasets)
- [Synthetic pre-training datasets](#synthetic-pre-training-datasets)
- [Training data preprocessing](#training-data-pre-processing)
- [Effect of pre-training data on downstream tasks](#effect-of-pre-training-data-on-downstream-tasks)
- [Bias and fairness issues in pre-training datasets](#bias-and-fairness-issues-in-pre-training-datasets)

§0.1. Decisions made during the pre-training process can heavily impact downstream performance.

§0.2. <mark>Model transparency</mark> is important – some proprietary LLMs do not make public information about how they work or how they were trained.

## Ingredients of an LLM

§1.1. The basic ingredients of an LLM are:
- **pre-training data** – What is the LLM trained on? What are the popular pre-training datasets? How is the training data <mark>pre-processed</mark> to ensure high-quality input.
- **vocabulary and tokeniser** – What is the LLM trained over? <mark>Mismatches</mark> between human-perceived words and LLM tokens can have an impact on downstream tasks.
- **learning objective** – What is the LLM being pre-trained to do? This may impact downstream tasks.
- **architecture** – What is the internal structure of the LLM? What are the components and how to they connect and interact? Every architecture has its own <mark>inductive bias</mark> – assumptions about the data and tasks.

§1.2. <mark>Data sources</mark> are collected, cleaned and filtered to create the <mark>pre-training dataset</mark>, which is then trained on the learning objective to create a trained <mark>base model</mark>, underpinned by a vocabulary.

§1.3. Base models can then be <mark>fine-tuned</mark> on a much smaller dataset to align them more with human needs and preferences:
- **supervised instruction fine tuning** – better at following human instructions
- **reinforcement learning by human feedback** (RLHF) – more aligned with human preferences
- **domain-adaptive (task-adaptive) continued pre-training** – better attuned to specific domains and tasks

§1.4. Fine-tuned LLM can be called *instruct models*, *chat models*, *domain-adapted models*, etc.

Back up to: [Top](#)

## Pre-training data requirements

§2.1. Today’s LLMs remain very <mark>sample inefficient</mark> – they need tons of examples to learn a task.

§2.2. Since supervised learning using human-annotated data is infeasible, LLMs are predominantly pre-trained using <mark>self-supervised learning</mark>, where the target labels exist within the training inputs.

§2.3. Is there enough text data in the world to learn all the skills that we want LLMs to learn, from form alone? How can we <mark>ground</mark> LLMs in the real world?

§2.4. Research is being done on whether <mark>multimodal models</mark> can provide a better approach to grounding.

§2.5. Can LLMs learn how to do <mark>reasoning</mark> from textual data alone? Can **process supervision** help (providing feedback for each step of the problem-solving process), rather than just **outcome supervision** (feedback on the final solution only)?

§2.6. Even the best LLMs still struggle with ambiguity. Is scaling up models and data enough here?

§2.7. Experiments are heppning with using <mark>synthetic data</mark> to pre-train models – text generated by LLMs. This is risky.

§2.8. LLMs can be trained for up to (or no more than!) five epochs without losing performance through **overfitting**.

Back up to: [Top](#)

## Popular pre-training datasets

§3.1.

Common Crawl (C4)

The Pile

WebText / OpenWebText / OpenWebText2

Wikipedia

BooksCorpus / BooksCorpus2

FineWeb


Training data is disappearing

Copyright issues

Back up to: [Top](#)

## Synthetic pre-training datasets

§4.1.

Back up to: [Top](#)

## Training data pre-processing

§5.1. Data pre-processing is unglamorous but hugely important for downstream tasks.

§5.2. Any pre-processing step should be very efficient, ideally linear time.

### Data filtering and cleaning

§5.3. <mark>HTML boilerplate</mark> text needs to be identified and removed – website menu text etc. One way of doing this is to only retain lines that end in sentence-ending punctuation. For example, you could run the web-extracted text (WET) version of the Common Crawl dataset through the `jusText` library. Other libraries are: `Dragnet`, `html2text`, `inscriptis`, `Newspaper`, `Trafilatura`, or (even better) a combination of all of these.

§5.4. Code blocks, tables, math formulas and image `alt` attributes all need careful preprocessing.

§5.5. The experimental Hypertext Language Model (HTLM) was trained on a ‘minified’ version of <mark>raw HTML</mark> (with some but not all tags removed).

§5.6. Other kinds of text also need to be identified and removed:
- <mark>non-English text</mark> – using libraries like `langdetect`, `langid`, `fasttext`, `pycld2`
- <mark>SEO spam</mark> – eg. documents with a low proportion of closed class words, which is a common characteristic of SEO tricks like ‘keyword stuffing’
- <mark>pornographic/abusive/toxic/hateful</mark> text – using keyword lists like the “List of dirty, naughty, obscene or otherwise bad words.” Toxic/hateful language

### Selecting quality documents

§5.7. One method for separating high-quality (eg. physics textbooks) from low-quality (eg. promotional text about a footwear brand) data is <mark>token-distribution Kullback-Liebler divergence</mark>, which identifies documents that have a lot of outlier tokens.

§5.8. Another approach is to build a classifier using Wikipedia as an example of high-quality data. Meta used this approach for training its Llama 2 and Llama 3 LLMs using `fasttext`.

§5.9. Another approach is to use <mark>perplexity</mark> – a measure of how well a model predicts a given text – after training a 5-gram language model on Wikipedia using `KenLM`. <mark>Perplexity sampling</mark> doesn’t just filter out high-perplexity documents, but rather oversamples from the middle part of the perplexity probability distribution.

### Deduplication

§5.10. <mark>Document-level duplicates</mark> (both exact and approximate) are generally removed before pre-training (unless there is an explicit need to use explicit duplicates to ensure oversampling of particular genres.).

§5.11 <mark>Sequence-level duplicates</mark> involve things like terms of service text, copyright notices, website prefaces, etc.

§5.12. Deduplication is usually performed using the <mark>MinHash</mark> algorithm.

§5.13. Benefits of deduplication are:
- Reduce overlap between training and test sets, allowing unbiased evaluation.
- Reduce the size of the training data (and hence time/compute required to train) whilst preserving overall perplexity.
- Reduce the tendency to memorise and regurgitate the training data (overfitting).

§5.14. Memorisation can create <mark>security vulnerabilities</mark> in LLMs:
- **membership inference attacks** – An attacker can determine if a particular sequence of text has been used to train the model or not.
- **training data extraction attacks** – An attacker can prompt the model to generate memorised sensitive information (phone numbers etc).

### Removing personally identifiable information

§5.15.

PII detection

PII remediation



### Training set decontamination

### Data mixtures

Back up to: [Top](#)

## Effect of pre-training data on downstream tasks

Back up to: [Top](#)

## Bias and fairness issues in pre-training datasets

Back up to: [Top](#)

----

Back up to: [Top](index.md) | [LLMs](../index.md) | [Artificial Intelligence](../../index.md)
