# Designing LLM Applications – Pre-training data

Contents:
- [Ingredients of an LLM](#ingredients-of-an-llm)
- [Pre-training data requirements](#pre-training-data-requirements)
- [Popular pre-training datasets](#popular-pre-training-datasets)
- [Synthetic pre-training datasets](#synthetic-pre-training-datasets)
- [Training data preprocessing](#training-data-pre-processing)
- [Effect of pre-training data on downstream tasks](#effect-of-pre-training-data-on-downstream-tasks)
- [Bias and fairness issues in pre-training datasets](#bias-and-fairness-issues-in-pre-training-datasets)

§0.1. Decisions made during the pre-training process can heavily impact downstream performance.

§0.2. <mark>Model transparency</mark> is important – some proprietary LLMs do not make public information about how they work or how they were trained.

## Ingredients of an LLM

§1.1. The basic ingredients of an LLM are:
- **pre-training data** – What is the LLM trained on? What are the popular pre-training datasets? How is the training data <mark>pre-processed</mark> to ensure high-quality input.
- **vocabulary and tokeniser** – What is the LLM trained over? <mark>Mismatches</mark> between human-perceived words and LLM tokens can have an impact on downstream tasks.
- **learning objective** – What is the LLM being pre-trained to do? This may impact downstream tasks.
- **architecture** – What is the internal structure of the LLM? What are the components and how to they connect and interact? Every architecture has its own <mark>inductive bias</mark> – assumptions about the data and tasks.

§1.2. <mark>Data sources</mark> are collected, cleaned and filtered to create the <mark>pre-training dataset</mark>, which is then trained on the learning objective to create a trained <mark>base model</mark>, underpinned by a vocabulary.

§1.3. Base models can then be <mark>fine-tuned</mark> on a much smaller dataset to align them more with human needs and preferences:
- **supervised instruction fine tuning** – better at following human instructions
- **reinforcement learning by human feedback** (RLHF) – more aligned with human preferences
- **domain-adaptive (task-adaptive) continued pre-training** – better attuned to specific domains and tasks

§1.4. Fine-tuned LLM can be called *instruct models*, *chat models*, *domain-adapted models*, etc.

Back up to: [Top](#)

## Pre-training data requirements

§2.1. Today’s LLMs remain very <mark>sample inefficient</mark> – they need tons of examples to learn a task.

§2.2. Since supervised learning using human-annotated data is infeasible, LLMs are predominantly pre-trained using <mark>self-supervised learning</mark>, where the target labels exist within the training inputs.

§2.3. Is there enough text data in the world to learn all the skills that we want LLMs to learn, from form alone? How can we <mark>ground</mark> LLMs in the real world?

§2.4. Research is being done on whether <mark>multimodal models</mark> can provide a better approach to grounding.

§2.5. Can LLMs learn how to do <mark>reasoning</mark> from textual data alone? Can **process supervision** help (providing feedback for each step of the problem-solving process), rather than just **outcome supervision** (feedback on the final solution only)?

§2.6. Even the best LLMs still struggle with ambiguity. Is scaling up models and data enough here?

§2.7. Experiments are heppning with using <mark>synthetic data</mark> to pre-train models – text generated by LLMs. This is risky.

§2.8. LLMs can be trained for up to (or no more than!) five epochs without losing performance through **overfitting**.

Back up to: [Top](#)

## Popular pre-training datasets

§3.1. <mark>Common Crawl</mark> (2019) is a very coarse dataset consisting of monthly snapshots of all web-crawl data. Google created a cleaned-up version – <mark>Colossal Cleaned Crawl Corpus</mark> (C4) – a 750Gb English language corpus. This is used by many LLMs, including all from the T5 family.

§3.2. <mark>The Pile</mark> (2020) is a diverse 825Gb dataset from Eleuther AI – C4, PubMed Central, arXiv, GitHub, FreeLaw Project, Stack Exchange, US Patent Office, Ubuntu IRC, HackerNews, YouTube, PhilPapers, Project Gutenberg, Wikipedia etc. Used by many LLMs, including Llama.

§3.3. <mark>WebText</mark> consists of webpages representing outbound links from Reddit that are relatively upvoted. Other versions are **OpenWebText** and **OpenWebText2** (65Gb, 2020). Used by GPT-2 and GPT-3.

§3.4. <mark>Wikipedia</mark> provides valuable, reliable factual knowledge to any LLM, but needs to be combined with other datasets to allow for more diversity of style.

§3.5. <mark>BooksCorpus</mark> (74M sentences, 2015) is the most influential of pre-training datasets, used by BERT, RoBERTa, GPT-2/3, etc, and contains over 7,000 books, mostly fiction, by unpublished authors, heavily weighted towards romance. Another version is BooksCorpus2, included in The Pile.

§3.6. <mark>FineWeb</mark>, published by Hugging Face, is the world’s largest publicly available pre-training dataset, with 15 trillion tokens, taken from 96 snapshots of Common Crawl, and heavily cleaned. See also **FineWeb-Edu**, for educational texts.

§3.7. Some other commonly used pre-training datasets are:
- **RedPajama** – 1.2T tokens, 2023
- **ROOTS** – 1.6T tokens, 2022
- **RefinedWeb** – 5T tokens, 2023
- **SlimPajama** – 627B tokens, 2023

§3.8. Training data is disappearing from the internet! Many of the above datasets are embroiled in <mark>copyright</mark> issues, and many websites are re-writing their terms of service to disallow training of LLMs. However, many builders of LLMs claim that this is protected as ‘fair use.

Back up to: [Top](#)

## Synthetic pre-training datasets

§4.1. Data generated by an LLM can be used to train another LLM.

§4.2. MicroSoft’s Phi models are heavily pre-trained on synthetic data.

§4.3. Hugging Face’s Cosmopedia is an open-source synthetic dataset, generated by Mistral LLM, and used to train the SmolLM models.

Back up to: [Top](#)

## Training data pre-processing

§5.1. Data pre-processing is unglamorous but hugely important for downstream tasks.

§5.2. Any pre-processing step should be very efficient, ideally linear time.

### Data filtering and cleaning

§5.3. <mark>HTML boilerplate</mark> text needs to be identified and removed – website menu text etc. One way of doing this is to only retain lines that end in sentence-ending punctuation. For example, you could run the web-extracted text (WET) version of the Common Crawl dataset through the `jusText` library. Other libraries are: `Dragnet`, `html2text`, `inscriptis`, `Newspaper`, `Trafilatura`, or (even better) a combination of all of these.

§5.4. Code blocks, tables, math formulas and image `alt` attributes all need careful preprocessing.

§5.5. The experimental Hypertext Language Model (HTLM) was trained on a ‘minified’ version of <mark>raw HTML</mark> (with some but not all tags removed).

§5.6. Other kinds of text also need to be identified and removed:
- <mark>non-English text</mark> – using libraries like `langdetect`, `langid`, `fasttext`, `pycld2`
- <mark>SEO spam</mark> – eg. documents with a low proportion of closed class words, which is a common characteristic of SEO tricks like ‘keyword stuffing’
- <mark>pornographic/abusive/toxic/hateful</mark> text – using keyword lists like the “List of dirty, naughty, obscene or otherwise bad words.” Toxic/hateful language

### Selecting quality documents

§5.7. One method for separating high-quality (eg. physics textbooks) from low-quality (eg. promotional text about a footwear brand) data is <mark>token-distribution Kullback-Liebler divergence</mark>, which identifies documents that have a lot of outlier tokens.

§5.8. Another approach is to build a classifier using Wikipedia as an example of high-quality data. Meta used this approach for training its Llama 2 and Llama 3 LLMs using `fasttext`.

§5.9. Another approach is to use <mark>perplexity</mark> – a measure of how well a model predicts a given text – after training a 5-gram language model on Wikipedia using `KenLM`. <mark>Perplexity sampling</mark> doesn’t just filter out high-perplexity documents, but rather oversamples from the middle part of the perplexity probability distribution.

### Deduplication

§5.10. <mark>Document-level duplicates</mark> (both exact and approximate) are generally removed before pre-training (unless there is an explicit need to use explicit duplicates to ensure oversampling of particular genres.).

§5.11 <mark>Sequence-level duplicates</mark> involve things like terms of service text, copyright notices, website prefaces, etc.

§5.12. Deduplication is usually performed using the <mark>MinHash</mark> algorithm.

§5.13. Benefits of deduplication are:
- Reduce overlap between training and test sets, allowing unbiased evaluation.
- Reduce the size of the training data (and hence time/compute required to train) whilst preserving overall perplexity.
- Reduce the tendency to memorise and regurgitate the training data (overfitting).

§5.14. Memorisation can create <mark>security vulnerabilities</mark> in LLMs:
- **membership inference attacks** – An attacker can determine if a particular sequence of text has been used to train the model or not.
- **training data extraction attacks** – An attacker can prompt the model to generate memorised sensitive information (phone numbers etc).

### Removing personally identifiable information

§5.15. Even with deduplication, memorisation of PII remains a concern. Training datasets may contain PII, which will need to be detected and remediated – names, addresses, credit card numbers, government IDs, medical history, email addresses, phone numbers, identity and affinity groups, geolocation, etc.

§5.16. However, we may want to retain <mark>public figure</mark> PII, so as to be able to answer general knowledge questions.

§5.17. Hackers can use membership inference or training data extraction attacks to target PII.

§5.18. <mark>Training data poisoning attacks</mark> – creating malicious websites in an attempt to influence the behaviour of an LLM trained on web-crawled text.

§5.19. <mark>LLM SEO</mark> – the art of creating websites that are more likely to be selected to pre-train LLMs.

§5.20. PII detection is similar to the named entity recognition (NER) task of NLP.

§5.21. PII remediation options are:
- replace with a special token, eg. `<phone number>`
- replace with a random token of the same entity type (eg. using the `Faker` library)
- replace with a shuffled token (of the same entity type?)
- remove the entire document (or the entire data source)

§5.22. Other approaches to preventing leakage of PII from LLMs are:
- differential privacy – introducing randomness in LLM inputs or outputs
- adversarial training
- model unlearning
- retroactive censoring
- memfree decoding.

### Training set decontamination

§5.23. A training dataset is contaminated if it contains data from <mark>benchmark test sets</mark> used to evaluate LLMs.

§5.24. Training sets are generally decontaminated using n-gram methods.

### Data mixtures

§5.25. LLMs need to be pre-trained on the right mixtures of data, with optimal proportions, using **upsampling** if necessary.

§5.26. Meta used the following mixture for Llama 3 – 50% general knowledge; 25% maths and reasoning; 17% code; and 8% non-English tokens.

§5.27. Other approaches for calculating data mixtures are `DoReMi` and `RegMix`.

§5.28. The order in which data is used to train LLMs is also important – this is called <mark>curriculum learning</mark>. One approach is to start by training on shorter sequences before moving on to longer ones.

Back up to: [Top](#)

## Effect of pre-training data on downstream tasks

§6.1. LLMs perform better at tasks that are more frequently represented in the pre-training dataset – eg. They are better at base 10 addition than at base 9 addition. They are better at sorting alphabetically than reverse alphabetically.

Back up to: [Top](#)

## Bias and fairness issues in pre-training datasets

§7.1. Given the scale of the pre-training datasets, LLMs are also models of the world we inhabit. But do we want to model the worlds as it is, or as we would like it to be.

§7.2. LLM suffer from <mark>bias amplification</mark> – they make biased predictions about groups of people at a higher rate than is found in their training data.

§7.3. Can we ‘fix’ the training data to model a world that suits our values and principles, and that downstream tasks can inherit? Note that the vast majority of contributors to Wikipedia and Reddit are men. Or should these problems be addressed downstream?

Back up to: [Top](#)

----

Back up to: [Top](index.md) | [LLMs](../index.md) | [Artificial Intelligence](../../index.md)
