# Designing LLM Applications – Pre-training data

Contents:
- [Ingredients of an LLM](#ingredients-of-an-llm)
- [Pre-training data requirements](#pre-training-data-requirements)
- [Popular pre-training datasets](#popular-pre-training-datasets)
- [Synthetic pre-training datasets](#synthetic-pre-training-datasets)
- [Training data preprocessing](#training-data-preprocessing)
- [Effect of pre-training data on downstream tasks](#effect-of-pre-training-data-on-downstream-tasks)
- [Bias and fairness issues in pre-training datasets](#bias-and-fairness-issues-in-pre-training-datasets)

§0.1. Decisions made during the pre-training process can heavily impact downstream performance.

§0.2. <mark>Model transparency</mark> is important – some proprietary LLMs do not make public information about how they work or how they were trained.

## Ingredients of an LLM

§1.1. The basic ingredients of an LLM are:
- **pre-training data** – What is the LLM trained on? What are the popular pre-training datasets? How is the training data <mark>pre-processed</mark> to ensure high-quality input.
- **vocabulary and tokeniser** – What is the LLM trained over? <mark>Mismatches</mark> between human-perceived words and LLM tokens can have an impact on downstream tasks.
- **learning objective** – What is the LLM being pre-trained to do? This may impact downstream tasks.
- **architecture** – What is the internal structure of the LLM? What are the components and how to they connect and interact? Every architecture has its own <mark>inductive bias</mark> – assumptions about the data and tasks.

§1.2. <mark>Data sources</mark> are collected, cleaned and filtered to create the <mark>pre-training dataset</mark>, which is then trained on the learning objective to create a trained <mark>base model</mark>, underpinned by a vocabulary.

§1.3. Base models can then be <mark>fine-tuned</mark> on a much smaller dataset to align them more with human needs and preferences:
- **supervised instruction fine tuning** – better at following human instructions
- **reinforcement learning by human feedback** (RLHF) – more aligned with human preferences
- **domain-adaptive (task-adaptive) continued pre-training** – better attuned to specific domains and tasks

§1.4. Fine-tuned LLM can be called *instruct models*, *chat models*, *domain-adapted models*, etc.

Back up to: [Top](#)

## Pre-training data requirements

§2.1. Today’s LLMs remain very <mark>sample inefficient</mark> – they need tons of examples to learn a task.

§2.2. Since supervised learning using human-annotated data is infeasible, LLMs are predominantly pre-trained using <mark>self-supervised learning</mark>, where the target labels exist within the training inputs.

§2.3. Is there enough text data in the world to learn all the skills that we want LLMs to learn, from form alone? How can we <mark>ground</mark> LLMs in the real world?

§2.4. Research is being done on whether <mark>multimodal models</mark> can provide a better approach to grounding.

§2.5. Can LLMs learn how to do <mark>reasoning</mark> from textual data alone? Can **process supervision** help (providing feedback for each step of the problem-solving process), rather than just **outcome supervision** (feedback on the final solution only)?

§2.6. Even the best LLMs still struggle with ambiguity. Is scaling up models and data enough here?

§2.7. Experiments are heppning with using <mark>synthetic data</mark> to pre-train models – text generated by LLMs. This is risky.

§2.8. LLMs can be trained for up to (or no more than!) five epochs without losing performance through **overfitting**.

Back up to: [Top](#)

## Popular pre-training datasets

§3.1.

Common Crawl (C4)

The Pile

WebText / OpenWebText / OpenWebText2

Wikipedia

BooksCorpus / BooksCorpus2

FineWeb


Training data is disappearing

Copyright issues

Back up to: [Top](#)

## Synthetic pre-training datasets

§4.1.

Back up to: [Top](#)

## Training data pre-processing

§5.1. Data pre-processing is unglamorous but hugely important for downstream tasks.

§5.2. Any pre-processing step should be very efficient, ideally linear time.

### Data filtering and cleaning

§5.3. <mark>HTML boilerplate</mark> text needs to be identified and removed – website menu text etc. One way of doing this is to only retain lines that end in sentence-ending punctuation. For example, you could run the web-extracted text (WET) version of the Common Crawl dataset through the `jusText` library. Other libraries are: `Dragnet`, `html2text`, `inscriptis`, `Newspaper`, `Trafilatura`, or (even better) a combination of all of these.

§5.4. Code blocks, tables, math formulas and image `alt` attributes all need careful preprocessing.

§5.5. The experimental Hypertext Language Model (HTLM) was trained on a ‘minified’ version of <mark>raw HTML</mark> (with some but not all tags removed).

§5.6. Other kinds of text also need to be identified and removed:
- <mark>non-English text</mark> – using libraries like `langdetect`, `langid`, `fasttext`, `pycld2`
- <mark>SEO spam</mark> – eg. documents with a low proportion of closed class words, which is a common characteristic of SEO tricks like ‘keyword stuffing’
- <mark>pornographic/abusive/toxic/hateful</mark> text – using keyword lists like the “List of dirty, naughty, obscene or otherwise bad words.” Toxic/hateful language

### Selecting quality documents

§5.7. 

Token-distribution K-L divergence

Classifier-based approaches

Perplexity for quality selection

### Deduplication

### Removing personally identifiable information

### Training set decontamination

### Data mixtures

Back up to: [Top](#)

## Effect of pre-training data on downstream tasks

Back up to: [Top](#)

## Bias and fairness issues in pre-training datasets

Back up to: [Top](#)

----

Back up to: [Top](index.md) | [LLMs](../index.md) | [Artificial Intelligence](../../index.md)
