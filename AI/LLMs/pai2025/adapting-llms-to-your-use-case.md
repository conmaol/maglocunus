# Adapting LLMs to your use case

Contents:
- [Navigating the LLM landscape](#navigating-the-llm-landscape)
- [How to choose an LLM for your task](#how-to-choose-an-llm-for-your-task)
- [Loading LLMs](#loading-llms)
- [Decoding strategies](#decoding-strategies)
- [Running inference on LLMs](#running-inference-on-llms)
- [Structured outputs](#structured-outputs)
- [Model debugging and interpretability](#model-debugging-and-interpretability)

## Navigating the LLM landscape

### Who are the LLM providers?

Types of LLM provider:
- companies providing proprietary LLMs
  - OpenAI (GPT), Google (Gemini), Anthropic (Claude), Cohere, AI21 etc.
  - API endpoints – LLM-as-a-service
  - often parner with cloud providers to provide fullt-managed services: Amazon Bedrock and SageMaker JumpStart, Google’s Vertex AI, Microsoft’s Azure OpenAI. 
- companies providing open source LLMs
  - companies that make LLM weights public and monetise by providing deployment services (TogetherAI)
  - companies whose primary business depends on AI adoption (Cerebras)
  - commercial research labs – Microsoft, Google, Meta, Salesforce etc.
- self-organising open source collectives and community research organisations
  - Eleuther AI, Big Science
- academia and government
  - Abu Dhabi’s Technology Innovation Institute, Tsinghua University

Major players:
- Google
  - BERT, MobileBERT, T5, FLAN-T5, ByT5, Canine, UL2, Flan-UL2, Pegasus PaLM, PaLMV2, ELECTRA, Tapas, Switch
- Microsoft
  - DeBERTa, DialoGPT, BioGPT, MPNet
- OpenAI
  - GPT-2, GPT-3, GPT-3.5, GPT-4
- Anthropic
  - Claude, Claude-2
- Meta
  - RoBERTa, Llama, Llama 2, BART, OPT, Galactica
- Amazon
  - Titan
- X.AI
  - Grok
- Cohere
- Salesforce
- MosaicML
- Cerebras
- Databricks
- Stability AI
- Together AI
- Ontocord AI
- Eleuther AI
- Big Science
- Tsinghua University (GLM)
- Technology Innovation Institute (Falcon)
- UC Berkeley (OpenLLaMA)
- Adept AI
- Mistral AI
- AI21 Labs (Jurassic)

### Model flavours

### Open source LLMs

Back up to: [Top](#)

## How to choose an LLM for your task

### Open source versus proprietary LLMs

### LLM evaluation

Back up to: [Top](#)

## Loading LLMs

### Hugging Face Accelerate

### Ollama

### LLM inference APIs

Back up to: [Top](#)

## Decoding strategies

### Greedy decoding

### Beam search

### Top-k sampling

### Top-p sampling

Back up to: [Top](#)

## Running inference on LLMs

Back up to: [Top](#)

## Structured outputs

Back up to: [Top](#)

## Model debugging and interpretability

Back up to: [Top](#)

----

Back up to: [Top](index.md) | [LLMs](../index.md) | [Artificial Intelligence](../../index.md)
