# Tokenisation in LLMs

[Notes from Chapter 2 of *Hands-On Large Language Models* by Jay Alammar and Maarten Grootendorst (2024)]

§1. A <mark>tokeniser</mark> breaks down the input text prompt into smaller pieces/chunks – words or parts of words.
- eg. “Have the bards who preceded”: “Have”, “the”, “bards”, “who”, “preceded”
- These tokens are then turned into ‘embeddings’ – numeric representations (ie. vectors) capturing their meaning.

## How tokenisers prepare the inputs to the language model


## Downloading and running an LLM

## How does the tokeniser break down text?

## Word versus subword versus character versus byte tokens

## Comparing Trained LLM tokenisers

## Tokeniser properties




----

Back up to: [LLMs](index.md) | [Artificial Intelligence](../index.md)
