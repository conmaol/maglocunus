# Activation functions

Every artificial neuron has a characteristic activation function, which determines what the neuronâ€™s output should be, based on the sum of its weighted inputs.

Note that:
- The input to an activation function is a real number (positive or negative).
- The output of an activation function is also a real number (positive or negative).

Examples of commonly used activation functions are:
- the [standard logistic function](standard_logistic_function.md) (sigmoid)
- the [rectified linear unit](rectified_linear_unit.md) function (ReLU)
- the [softplus](softplus.md) function (a smoothed version of ReLU)
- the [hyperbolic tangent function](hyperbolic_tangent_function.md)

See also:
- Wikipedia page on [activation functions](https://en.wikipedia.org/wiki/Activation_function)

----

Back to: [Index](index.md)
