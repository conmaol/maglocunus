# Activation functions

Every artificial neuron has a characteristic activation function, which decides what the neuron’s output should be based on the sum of its weighted inputs.

Note that:
- The input to an activation function is a real number (positive or negative).
- The output of an activation function is also a real number (positive or negative).

Examples of activation functions are:
- [ReLU](ReLU.md) – the rectified linear unit function
- the [standard logistic function](standard_logistic_function.md)

See also:
- [Wikipedia page](https://en.wikipedia.org/wiki/Activation_function)

----

Back to: [Index](index.md)
