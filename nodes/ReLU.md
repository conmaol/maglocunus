# Rectified linear unit

The rectified linear unit (ReLU) function is an [activation function](activation_functions.md) for artificial neurons. 

ReLU is defined as the function $f$ such that:
- If $x<0$, then $f(x)=0$.
- Otherwise, $f(x) = x$.

The blue line in the following graph represents the ReLU function:

![ReLU graph](https://upload.wikimedia.org/wikipedia/commons/thumb/4/42/ReLU_and_GELU.svg/308px-ReLU_and_GELU.svg.png)

Note that the output of ReLU is never negative and there is no maximum value. ReLU is essentially just the identify function for non-negative inputs.

----

Back to: [Index](index.md)
